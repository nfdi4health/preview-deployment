apiVersion: batch/v1
kind: CronJob
metadata:
  name: {{ .Release.Name }}-backend-data-import
spec:
  schedule: "0 0 * * *"
  successfulJobsHistoryLimit: 0
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - image: {{ .Values.images.backend }}
            name: backend-download-data
            env:
              - name: ELASTIC_INDEX
                value: {{ .Values.oidc.backend.elasticindex }}
              - name: ELASTIC_URL
                value: http://{{ .Release.Name }}-elastic:9200
              - name: S3_KEY
                valueFrom:
                  secretKeyRef:
                    name: preview-s3-key
                    key: preview-s3-key
              - name: S3_KEYID
                valueFrom:
                  secretKeyRef:
                    name: preview-s3-keyid
                    key: preview-s3-keyid
              - name: S3_HOST
                value: {{ .Values.oidc.data_import.host }}
              - name: S3_BUCKET
                value: {{ .Values.oidc.data_import.bucket }}
            command: [ "/bin/sh","-c" ]
            args: [ "s3cmd get s3://${S3_BUCKET}/final.jsonl --host=${S3_HOST} --access_key=${S3_KEYID} --secret_key=${S3_KEY} --host-bucket=${S3_BUCKET}.${S3_HOST}; python3 /project/import_elastic.py ${ELASTIC_URL} ${ELASTIC_INDEX} final.jsonl" ]
            resources:
              requests:
                memory: 1900Mi
                cpu: 8m
              limits:
                memory: 3800Mi
                cpu: 16m
          restartPolicy: Never
          imagePullSecrets:
            - name: preview-dockerconfigjson-github-com
      backoffLimit: 4


